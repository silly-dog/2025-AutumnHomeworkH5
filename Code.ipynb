{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc16bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "train size: 4000 missing: 0\n",
      "test  size: 511 missing: 0\n",
      "train_split: 3600 val_split: 400\n",
      "val label dist:\n",
      " tag\n",
      "positive    0.5975\n",
      "negative    0.2975\n",
      "neutral     0.1050\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\26930\\anaconda3\\envs\\pytorch-cuda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 29053c32-c134-4e3b-b6c3-093f9ec694e4)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP finetune enabled\n",
      "[WARN] CLIP pipeline failed, fallback to light model. Error: ValueError('Attempting to unscale FP16 gradients.')\n",
      "[fallback] vocab size: 4353\n",
      "[fallback] Epoch 1/10 | loss=1.1183 | val_acc=0.4850 | val_mf1=0.3033\n",
      "[fallback] Epoch 2/10 | loss=1.0820 | val_acc=0.3650 | val_mf1=0.3537\n",
      "[fallback] Epoch 3/10 | loss=1.0126 | val_acc=0.4500 | val_mf1=0.4047\n",
      "[fallback] Epoch 4/10 | loss=0.8882 | val_acc=0.5300 | val_mf1=0.4425\n",
      "[fallback] Epoch 5/10 | loss=0.7777 | val_acc=0.4975 | val_mf1=0.4449\n",
      "[fallback] Epoch 6/10 | loss=0.6364 | val_acc=0.5925 | val_mf1=0.4807\n",
      "[fallback] Epoch 7/10 | loss=0.4853 | val_acc=0.5875 | val_mf1=0.5008\n",
      "[fallback] Epoch 8/10 | loss=0.3697 | val_acc=0.5425 | val_mf1=0.4612\n",
      "[fallback] Epoch 9/10 | loss=0.2693 | val_acc=0.6150 | val_mf1=0.5019\n",
      "[fallback] Epoch 10/10 | loss=0.2111 | val_acc=0.6200 | val_mf1=0.4930\n",
      "[fallback] best val macro-F1: 0.5018982808998249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      0.55      0.53       119\n",
      "     neutral       0.27      0.19      0.22        42\n",
      "    positive       0.71      0.73      0.72       239\n",
      "\n",
      "    accuracy                           0.62       400\n",
      "   macro avg       0.50      0.49      0.49       400\n",
      "weighted avg       0.61      0.62      0.61       400\n",
      "\n",
      "saved: C:\\Users\\26930\\Desktop\\当代人工智能\\Project5\\project5\\submission.csv\n",
      "   guid       tag\n",
      "0     8  positive\n",
      "1  1576  positive\n",
      "2  2320  positive\n",
      "3  4912  positive\n",
      "4  3821  positive\n"
     ]
    }
   ],
   "source": [
    "# 多模态情感分类（3类）：text(.txt)+image(.jpg) -> {negative, neutral, positive}\n",
    "# 调优版：优先使用 CLIP（文本+图像预训练编码）提升准确率；若无法下载模型则自动回退到轻量模型。\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# ============== 基础配置 ==============\n",
    "SEED = 42\n",
    "def seed_everything(seed: int = SEED) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device =\", device)\n",
    "\n",
    "# 让 CUDA 上更快一些（不影响结果正确性）\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "TRAIN_CSV = ROOT / \"train.txt\"\n",
    "TEST_CSV = ROOT / \"test_without_label.txt\"\n",
    "\n",
    "assert DATA_DIR.exists(), f\"找不到数据目录: {DATA_DIR}\"\n",
    "assert TRAIN_CSV.exists(), f\"找不到: {TRAIN_CSV}\"\n",
    "assert TEST_CSV.exists(), f\"找不到: {TEST_CSV}\"\n",
    "\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# ============== 读取数据 ==============\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "for df_name, df in [(\"train\", train_df), (\"test\", test_df)]:\n",
    "    assert \"guid\" in df.columns and \"tag\" in df.columns, f\"{df_name} 缺少列 guid/tag\"\n",
    "    df[\"guid\"] = df[\"guid\"].astype(str)\n",
    "\n",
    "train_df = train_df[train_df[\"tag\"].isin(label2id)].copy()\n",
    "train_df[\"label\"] = train_df[\"tag\"].map(label2id).astype(int)\n",
    "\n",
    "def add_paths(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"text_path\"] = df[\"guid\"].apply(lambda g: str(DATA_DIR / f\"{g}.txt\"))\n",
    "    df[\"img_path\"]  = df[\"guid\"].apply(lambda g: str(DATA_DIR / f\"{g}.jpg\"))\n",
    "    return df\n",
    "\n",
    "train_df = add_paths(train_df)\n",
    "test_df = add_paths(test_df)\n",
    "\n",
    "missing_train = train_df[~train_df[\"text_path\"].map(os.path.exists) | ~train_df[\"img_path\"].map(os.path.exists)]\n",
    "missing_test  = test_df[~test_df[\"text_path\"].map(os.path.exists)  | ~test_df[\"img_path\"].map(os.path.exists)]\n",
    "print(\"train size:\", len(train_df), \"missing:\", len(missing_train))\n",
    "print(\"test  size:\", len(test_df),  \"missing:\", len(missing_test))\n",
    "assert len(missing_train) == 0, \"训练集中存在缺失的txt/jpg文件\"\n",
    "assert len(missing_test) == 0, \"测试集中存在缺失的txt/jpg文件\"\n",
    "\n",
    "# ============== 划分验证集（可调整） ==============\n",
    "VAL_RATIO = 0.1\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(train_df)),\n",
    "    test_size=VAL_RATIO,\n",
    "    random_state=SEED,\n",
    "    shuffle=True,\n",
    "    stratify=train_df[\"label\"],\n",
    ")\n",
    "train_split = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "val_split = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "print(\"train_split:\", len(train_split), \"val_split:\", len(val_split))\n",
    "print(\"val label dist:\\n\", val_split[\"tag\"].value_counts(normalize=True))\n",
    "\n",
    "# ============== 训练超参数（可调，偏向更高准确率） ==============\n",
    "MAX_LEN = 96\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32 if device.type == \"cuda\" else 16\n",
    "EPOCHS = 10 if device.type == \"cuda\" else 6\n",
    "NUM_WORKERS = 0  # Windows/Notebook 用0最稳\n",
    "USE_AMP = device.type == \"cuda\"\n",
    "\n",
    "# 训练策略（可调）\n",
    "GRAD_ACCUM_STEPS = 2 if device.type == \"cuda\" else 1\n",
    "PATIENCE = 3  # early stopping\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# ============== I/O 工具 ==============\n",
    "def load_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read().strip()\n",
    "\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "# ============== 尝试使用 CLIP（更强的多模态预训练） ==============\n",
    "USE_CLIP = True\n",
    "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "FREEZE_CLIP = True if device.type == \"cpu\" else False  # CPU下建议冻结以加速\n",
    "HEAD_LR = 1e-3\n",
    "FULL_LR = 1e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    for batch in loader:\n",
    "        # 注意：不要把 labels 传进 model.forward\n",
    "        logits = model(**{k: v.to(device, non_blocking=True) for k, v in batch.items() if k not in (\"guid\", \"labels\")})\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(batch[\"labels\"].cpu())\n",
    "    y_pred = torch.cat(all_preds).numpy()\n",
    "    y_true = torch.cat(all_labels).numpy()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return acc, mf1, y_true, y_pred\n",
    "\n",
    "def _compute_class_weights(labels: list[int], num_classes: int) -> torch.Tensor:\n",
    "    # 反比于频次的权重，避免模型偏向多数类；再做归一化让平均权重约为1\n",
    "    counts = np.bincount(np.asarray(labels, dtype=np.int64), minlength=num_classes).astype(np.float32)\n",
    "    counts[counts == 0] = 1.0\n",
    "    inv = 1.0 / counts\n",
    "    inv = inv * (num_classes / inv.sum())\n",
    "    return torch.tensor(inv, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer,\n",
    "    scaler: torch.amp.GradScaler | None,\n",
    "    loss_fn: nn.Module,\n",
    "    grad_accum_steps: int = 1,\n",
    "):\n",
    "    model.train()\n",
    "    running, n = 0.0, 0\n",
    "    grad_accum_steps = max(int(grad_accum_steps), 1)\n",
    "    for batch in loader:\n",
    "        labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with torch.amp.autocast(device_type=\"cuda\", enabled=USE_AMP):\n",
    "                logits = model(**{k: v.to(device, non_blocking=True) for k, v in batch.items() if k not in (\"guid\", \"labels\")})\n",
    "                loss = loss_fn(logits, labels) / grad_accum_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(**{k: v.to(device, non_blocking=True) for k, v in batch.items() if k not in (\"guid\", \"labels\")})\n",
    "            loss = loss_fn(logits, labels) / grad_accum_steps\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        bs = labels.size(0)\n",
    "        running += (loss.item() * grad_accum_steps) * bs\n",
    "        n += bs\n",
    "    return running / max(n, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_labels(model: nn.Module, loader: DataLoader) -> list[int]:\n",
    "    model.eval()\n",
    "    out = []\n",
    "    for batch in loader:\n",
    "        logits = model(**{k: v.to(device, non_blocking=True) for k, v in batch.items() if k not in (\"guid\", \"labels\")})\n",
    "        out.append(torch.argmax(logits, dim=-1).cpu())\n",
    "    return torch.cat(out).tolist()\n",
    "\n",
    "def run_clip_pipeline():\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "    # 关键：强制用 safetensors，避免 torch<2.6 时触发 torch.load 安全限制\n",
    "    processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME, use_fast=True)\n",
    "    clip = CLIPModel.from_pretrained(\n",
    "        CLIP_MODEL_NAME,\n",
    "        use_safetensors=True,\n",
    "        torch_dtype=(torch.float16 if device.type == \"cuda\" else None),\n",
    "    )\n",
    "\n",
    "    class MMDS_CLIP(Dataset):\n",
    "        def __init__(self, df: pd.DataFrame, with_label: bool):\n",
    "            self.df = df.reset_index(drop=True)\n",
    "            self.with_label = with_label\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx: int):\n",
    "            row = self.df.iloc[idx]\n",
    "            guid = row[\"guid\"]\n",
    "            text = load_text(row[\"text_path\"])\n",
    "            img = load_image(row[\"img_path\"])\n",
    "            item = {\"guid\": guid, \"text\": text, \"image\": img}\n",
    "            if self.with_label:\n",
    "                item[\"labels\"] = int(row[\"label\"])\n",
    "            return item\n",
    "\n",
    "    def collate_fn(batch: list[dict]):\n",
    "        texts = [b[\"text\"] for b in batch]\n",
    "        images = [b[\"image\"] for b in batch]\n",
    "        guids = [b[\"guid\"] for b in batch]\n",
    "        enc = processor(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        out = {\n",
    "            \"guid\": guids,\n",
    "            \"input_ids\": enc[\"input_ids\"],\n",
    "            \"attention_mask\": enc.get(\"attention_mask\", None),\n",
    "            \"pixel_values\": enc[\"pixel_values\"],\n",
    "        }\n",
    "        if out[\"attention_mask\"] is None:\n",
    "            out.pop(\"attention_mask\")\n",
    "        if \"labels\" in batch[0]:\n",
    "            out[\"labels\"] = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n",
    "        return out\n",
    "\n",
    "    train_ds = MMDS_CLIP(train_split, with_label=True)\n",
    "    val_ds = MMDS_CLIP(val_split, with_label=True)\n",
    "    test_ds = MMDS_CLIP(test_df, with_label=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "    class CLIPClassifier(nn.Module):\n",
    "        def __init__(self, clip: CLIPModel, num_classes: int = 3, dropout: float = 0.2):\n",
    "            super().__init__()\n",
    "            self.clip = clip\n",
    "            dim = clip.config.projection_dim  # usually 512\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(dim * 2, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(512, num_classes),\n",
    "            )\n",
    "\n",
    "        def forward(self, input_ids: torch.Tensor, pixel_values: torch.Tensor, attention_mask: torch.Tensor | None = None):\n",
    "            out = self.clip(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=pixel_values,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            # CLIP returns already projected embeddings\n",
    "            t = out.text_embeds\n",
    "            v = out.image_embeds\n",
    "            fused = torch.cat([t, v], dim=-1)\n",
    "            return self.head(fused)\n",
    "\n",
    "    model = CLIPClassifier(clip, num_classes=len(label2id)).to(device)\n",
    "\n",
    "    # 类别权重（缓解类别不平衡：neutral 很少）\n",
    "    class_w = _compute_class_weights(train_split[\"label\"].tolist(), num_classes=len(label2id)).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_w)\n",
    "\n",
    "    if FREEZE_CLIP:\n",
    "        for p in model.clip.parameters():\n",
    "            p.requires_grad = False\n",
    "        params = list(model.head.parameters())\n",
    "        lr = HEAD_LR\n",
    "        print(\"CLIP frozen: train head only\")\n",
    "    else:\n",
    "        params = model.parameters()\n",
    "        lr = FULL_LR\n",
    "        print(\"CLIP finetune enabled\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(params, lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # 学习率调度：余弦 + warmup（更稳）\n",
    "    try:\n",
    "        from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "        num_update_steps = max(1, (len(train_loader) // GRAD_ACCUM_STEPS) * EPOCHS)\n",
    "        num_warmup_steps = int(num_update_steps * WARMUP_RATIO)\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_update_steps)\n",
    "    except Exception:\n",
    "        scheduler = None\n",
    "\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=USE_AMP)\n",
    "\n",
    "    best_state, best_mf1 = None, -1.0\n",
    "    bad_epochs = 0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss = train_one_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            scaler if USE_AMP else None,\n",
    "            loss_fn=loss_fn,\n",
    "            grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "        )\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        acc, mf1, y_true, y_pred = evaluate(model, val_loader)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | loss={tr_loss:.4f} | val_acc={acc:.4f} | val_mf1={mf1:.4f}\")\n",
    "        if mf1 > best_mf1:\n",
    "            best_mf1 = mf1\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= PATIENCE:\n",
    "                print(f\"Early stopping triggered (patience={PATIENCE}).\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    acc, mf1, y_true, y_pred = evaluate(model, val_loader)\n",
    "    print(\"best val macro-F1:\", mf1, \"val_acc:\", acc)\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "    test_pred_ids = predict_labels(model, test_loader)\n",
    "    submission = pd.DataFrame({\n",
    "        \"guid\": test_df[\"guid\"].astype(str).values,\n",
    "        \"tag\": [id2label[i] for i in test_pred_ids],\n",
    "    })\n",
    "    out_path = ROOT / \"submission.csv\"\n",
    "    submission.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(\"saved:\", out_path)\n",
    "    print(submission.head())\n",
    "\n",
    "def run_fallback_light_model():\n",
    "    # 轻量回退模型（无torchvision、无外部下载），比之前略微调优：更强图像增强+更长训练\n",
    "    _token_pat = re.compile(r\"[A-Za-z]+(?:'[A-Za-z]+)?|\\d+|[^\\w\\s]\", re.UNICODE)\n",
    "    def basic_tokenize(s: str) -> list[str]:\n",
    "        s = (s or \"\").lower()\n",
    "        return _token_pat.findall(s)\n",
    "\n",
    "    def build_vocab(df: pd.DataFrame, max_vocab: int = 30000, min_freq: int = 2) -> dict[str, int]:\n",
    "        freq: dict[str, int] = {}\n",
    "        for p in df[\"text_path\"].tolist():\n",
    "            toks = basic_tokenize(load_text(p))\n",
    "            for t in toks:\n",
    "                freq[t] = freq.get(t, 0) + 1\n",
    "        items = [(t, c) for t, c in freq.items() if c >= min_freq]\n",
    "        items.sort(key=lambda x: x[1], reverse=True)\n",
    "        items = items[:max_vocab]\n",
    "        vocab = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
    "        for t, _ in items:\n",
    "            if t not in vocab:\n",
    "                vocab[t] = len(vocab)\n",
    "        return vocab\n",
    "\n",
    "    vocab = build_vocab(train_split, max_vocab=30000, min_freq=2)\n",
    "    pad_id = vocab[\"[PAD]\"]\n",
    "    unk_id = vocab[\"[UNK]\"]\n",
    "    print(\"[fallback] vocab size:\", len(vocab))\n",
    "\n",
    "    def encode_text(text: str, max_len: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        toks = basic_tokenize(text)\n",
    "        ids = [vocab.get(t, unk_id) for t in toks][:max_len]\n",
    "        attn = [1] * len(ids)\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.tensor(attn, dtype=torch.long)\n",
    "\n",
    "    img_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    img_std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "    def pil_to_tensor(img: Image.Image) -> torch.Tensor:\n",
    "        arr = np.asarray(img, dtype=np.float32) / 255.0\n",
    "        if arr.ndim == 2:\n",
    "            arr = np.stack([arr, arr, arr], axis=-1)\n",
    "        arr = np.transpose(arr, (2, 0, 1))\n",
    "        return torch.from_numpy(arr)\n",
    "\n",
    "    def image_transform(img: Image.Image, train: bool) -> torch.Tensor:\n",
    "        img = img.convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "        if train:\n",
    "            if random.random() < 0.5:\n",
    "                img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n",
    "            if random.random() < 0.5:\n",
    "                img = ImageEnhance.Brightness(img).enhance(1.0 + random.uniform(-0.15, 0.15))\n",
    "            if random.random() < 0.5:\n",
    "                img = ImageEnhance.Contrast(img).enhance(1.0 + random.uniform(-0.15, 0.15))\n",
    "            if random.random() < 0.35:\n",
    "                img = ImageEnhance.Color(img).enhance(1.0 + random.uniform(-0.15, 0.15))\n",
    "        x = pil_to_tensor(img)\n",
    "        x = (x - img_mean) / img_std\n",
    "        return x\n",
    "\n",
    "    class MMDS(Dataset):\n",
    "        def __init__(self, df: pd.DataFrame, train_img: bool, with_label: bool):\n",
    "            self.df = df.reset_index(drop=True)\n",
    "            self.train_img = train_img\n",
    "            self.with_label = with_label\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "\n",
    "        def __getitem__(self, idx: int):\n",
    "            row = self.df.iloc[idx]\n",
    "            guid = row[\"guid\"]\n",
    "            text = load_text(row[\"text_path\"])\n",
    "            input_ids, attention_mask = encode_text(text, MAX_LEN)\n",
    "            img = load_image(row[\"img_path\"])\n",
    "            img_t = image_transform(img, train=self.train_img)\n",
    "            item = {\"guid\": guid, \"input_ids\": input_ids, \"attention_mask\": attention_mask, \"image\": img_t}\n",
    "            if self.with_label:\n",
    "                item[\"labels\"] = torch.tensor(int(row[\"label\"]), dtype=torch.long)\n",
    "            return item\n",
    "\n",
    "    def collate(batch: list[dict]):\n",
    "        guids = [b[\"guid\"] for b in batch]\n",
    "        input_ids = pad_sequence([b[\"input_ids\"] for b in batch], batch_first=True, padding_value=pad_id)\n",
    "        attention_mask = pad_sequence([b[\"attention_mask\"] for b in batch], batch_first=True, padding_value=0)\n",
    "        images = torch.stack([b[\"image\"] for b in batch], dim=0)\n",
    "        out = {\"guid\": guids, \"input_ids\": input_ids, \"attention_mask\": attention_mask, \"image\": images}\n",
    "        if \"labels\" in batch[0]:\n",
    "            out[\"labels\"] = torch.stack([b[\"labels\"] for b in batch], dim=0)\n",
    "        return out\n",
    "\n",
    "    train_ds = MMDS(train_split, train_img=True, with_label=True)\n",
    "    val_ds   = MMDS(val_split,   train_img=False, with_label=True)\n",
    "    test_ds  = MMDS(test_df,     train_img=False, with_label=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, collate_fn=collate, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate, pin_memory=True)\n",
    "\n",
    "    class TextEncoder(nn.Module):\n",
    "        def __init__(self, vocab_size: int, emb_dim: int = 192, hidden: int = 192, dropout: float = 0.2):\n",
    "            super().__init__()\n",
    "            self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "            self.gru = nn.GRU(emb_dim, hidden, batch_first=True, bidirectional=True)\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.out_dim = hidden * 2\n",
    "\n",
    "        def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "            x = self.emb(input_ids)\n",
    "            x, _ = self.gru(x)\n",
    "            x = self.drop(x)\n",
    "            mask = attention_mask.unsqueeze(-1).float()\n",
    "            x = x * mask\n",
    "            denom = mask.sum(dim=1).clamp_min(1.0)\n",
    "            return x.sum(dim=1) / denom\n",
    "\n",
    "    class ImageEncoder(nn.Module):\n",
    "        def __init__(self, out_dim: int = 256):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            )\n",
    "            self.proj = nn.Linear(256, out_dim)\n",
    "\n",
    "        def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "            x = self.net(image).flatten(1)\n",
    "            return self.proj(x)\n",
    "\n",
    "    class Fusion(nn.Module):\n",
    "        def __init__(self, vocab_size: int, num_classes: int = 3, fusion_dim: int = 256, dropout: float = 0.25):\n",
    "            super().__init__()\n",
    "            self.text = TextEncoder(vocab_size=vocab_size)\n",
    "            self.img = ImageEncoder(out_dim=fusion_dim)\n",
    "            self.text_proj = nn.Sequential(nn.Linear(self.text.out_dim, fusion_dim), nn.ReLU(), nn.Dropout(dropout))\n",
    "            self.cls = nn.Sequential(\n",
    "                nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(fusion_dim, num_classes),\n",
    "            )\n",
    "\n",
    "        def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, image: torch.Tensor):\n",
    "            t = self.text_proj(self.text(input_ids, attention_mask))\n",
    "            v = self.img(image)\n",
    "            return self.cls(torch.cat([t, v], dim=-1))\n",
    "\n",
    "    model = Fusion(vocab_size=len(vocab), num_classes=len(label2id)).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=WEIGHT_DECAY)\n",
    "    class_w = _compute_class_weights(train_split[\"label\"].tolist(), num_classes=len(label2id)).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_w)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=USE_AMP)\n",
    "\n",
    "    best_state, best_mf1 = None, -1.0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        running, n = 0.0, 0\n",
    "        for batch in train_loader:\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type=\"cuda\", enabled=USE_AMP):\n",
    "                    logits = model(\n",
    "                        input_ids=batch[\"input_ids\"].to(device),\n",
    "                        attention_mask=batch[\"attention_mask\"].to(device),\n",
    "                        image=batch[\"image\"].to(device),\n",
    "                    )\n",
    "                    loss = loss_fn(logits, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                logits = model(\n",
    "                    input_ids=batch[\"input_ids\"].to(device),\n",
    "                    attention_mask=batch[\"attention_mask\"].to(device),\n",
    "                    image=batch[\"image\"].to(device),\n",
    "                )\n",
    "                loss = loss_fn(logits, labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            bs = labels.size(0)\n",
    "            running += loss.item() * bs\n",
    "            n += bs\n",
    "        tr_loss = running / max(n, 1)\n",
    "\n",
    "        # eval\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                logits = model(\n",
    "                    input_ids=batch[\"input_ids\"].to(device),\n",
    "                    attention_mask=batch[\"attention_mask\"].to(device),\n",
    "                    image=batch[\"image\"].to(device),\n",
    "                )\n",
    "                preds = torch.argmax(logits, dim=-1).cpu()\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(batch[\"labels\"])\n",
    "        y_pred = torch.cat(all_preds).numpy()\n",
    "        y_true = torch.cat(all_labels).numpy()\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        mf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        print(f\"[fallback] Epoch {epoch}/{EPOCHS} | loss={tr_loss:.4f} | val_acc={acc:.4f} | val_mf1={mf1:.4f}\")\n",
    "        if mf1 > best_mf1:\n",
    "            best_mf1 = mf1\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    print(\"[fallback] best val macro-F1:\", best_mf1)\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "    # predict\n",
    "    model.eval()\n",
    "    preds_all = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            logits = model(\n",
    "                input_ids=batch[\"input_ids\"].to(device),\n",
    "                attention_mask=batch[\"attention_mask\"].to(device),\n",
    "                image=batch[\"image\"].to(device),\n",
    "            )\n",
    "            preds_all.append(torch.argmax(logits, dim=-1).cpu())\n",
    "    test_pred_ids = torch.cat(preds_all).tolist()\n",
    "    submission = pd.DataFrame({\n",
    "        \"guid\": test_df[\"guid\"].astype(str).values,\n",
    "        \"tag\": [id2label[i] for i in test_pred_ids],\n",
    "    })\n",
    "    out_path = ROOT / \"submission.csv\"\n",
    "    submission.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(\"saved:\", out_path)\n",
    "    print(submission.head())\n",
    "\n",
    "# ============== 运行（优先CLIP；失败则回退） ==============\n",
    "if USE_CLIP:\n",
    "    try:\n",
    "        run_clip_pipeline()\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] CLIP pipeline failed, fallback to light model. Error:\", repr(e))\n",
    "        run_fallback_light_model()\n",
    "else:\n",
    "    run_fallback_light_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-cuda)",
   "language": "python",
   "name": "pytorch-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
